<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"arabelatso.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="最近在用OpenNMT训练机器翻译模型。OpenNMT 全称 Open-Source Neural Machine Translation，是https:&#x2F;&#x2F;nlp.seas.harvard.edu&#x2F; 和SYSTRAN 共同开发的适用于机器翻译的集成库（现在由SYSTRAN和 Ubiqus 维护）。 OpenNMT有两个版本，分别为依赖PyTorch 和TensorFlow的。从使用人数上来说，P">
<meta property="og:type" content="article">
<meta property="og:title" content="OpenNMT 2.0.0rc1 使用手册">
<meta property="og:url" content="https://arabelatso.github.io/2021/01/03/OpenNMT-Doc/index.html">
<meta property="og:site_name" content="Arabela&#39;s Blog">
<meta property="og:description" content="最近在用OpenNMT训练机器翻译模型。OpenNMT 全称 Open-Source Neural Machine Translation，是https:&#x2F;&#x2F;nlp.seas.harvard.edu&#x2F; 和SYSTRAN 共同开发的适用于机器翻译的集成库（现在由SYSTRAN和 Ubiqus 维护）。 OpenNMT有两个版本，分别为依赖PyTorch 和TensorFlow的。从使用人数上来说，P">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://arabelatso.github.io/OpenNMT-Doc/parallel.png">
<meta property="og:image" content="https://arabelatso.github.io/OpenNMT-Doc/orpus.png">
<meta property="og:image" content="https://arabelatso.github.io/OpenNMT-Doc/taboeba.png">
<meta property="article:published_time" content="2021-01-02T16:00:00.000Z">
<meta property="article:modified_time" content="2024-06-21T12:30:20.421Z">
<meta property="article:author" content="Arabela Tso">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://arabelatso.github.io/OpenNMT-Doc/parallel.png">


<link rel="canonical" href="https://arabelatso.github.io/2021/01/03/OpenNMT-Doc/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://arabelatso.github.io/2021/01/03/OpenNMT-Doc/","path":"2021/01/03/OpenNMT-Doc/","title":"OpenNMT 2.0.0rc1 使用手册"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>OpenNMT 2.0.0rc1 使用手册 | Arabela's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Arabela's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-OpenNMT-2-0-0rc"><span class="nav-number">1.</span> <span class="nav-text">Why OpenNMT 2.0.0rc</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Installation"><span class="nav-number">2.</span> <span class="nav-text">Installation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prerequisite"><span class="nav-number">2.1.</span> <span class="nav-text">Prerequisite</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#From-pip"><span class="nav-number">2.2.</span> <span class="nav-text">From pip</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#From-source"><span class="nav-number">2.3.</span> <span class="nav-text">From source</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Check-installation"><span class="nav-number">2.4.</span> <span class="nav-text">Check installation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-0-Training-data-preparation"><span class="nav-number">3.</span> <span class="nav-text">Step 0. Training data preparation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset-preparation"><span class="nav-number">3.1.</span> <span class="nav-text">Dataset preparation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset-Download"><span class="nav-number">3.2.</span> <span class="nav-text">Dataset Download</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#WMT-DATASET"><span class="nav-number">3.2.1.</span> <span class="nav-text">WMT DATASET</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPUS"><span class="nav-number">3.2.2.</span> <span class="nav-text">OPUS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tatoeba-Kaggle"><span class="nav-number">3.2.3.</span> <span class="nav-text">Tatoeba Kaggle</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optional-Preprocess"><span class="nav-number">3.3.</span> <span class="nav-text">(Optional) Preprocess</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-Build-Vocabulary"><span class="nav-number">4.</span> <span class="nav-text">Step 1. Build Vocabulary</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Example-configuration-for-Build-Vocab"><span class="nav-number">4.1.</span> <span class="nav-text">Example configuration for Build Vocab</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Transformer"><span class="nav-number">4.2.</span> <span class="nav-text">Data Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Preprocess-Filterlong"><span class="nav-number">4.2.1.</span> <span class="nav-text">Preprocess - Filterlong</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tokenization"><span class="nav-number">4.2.2.</span> <span class="nav-text">Tokenization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#No-special-tokenization"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">No special tokenization.</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sentencepiece"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">Sentencepiece</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BPE"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">BPE</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-Train-the-model"><span class="nav-number">5.</span> <span class="nav-text">Step 2. Train the model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-3-Translate"><span class="nav-number">6.</span> <span class="nav-text">Step 3. Translate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E"><span class="nav-number">7.</span> <span class="nav-text">写在最后</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Arabela Tso</p>
  <div class="site-description" itemprop="description">May all your optimal is global.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ArabelaTso" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ArabelaTso" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://arabelatso.github.io/2021/01/03/OpenNMT-Doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Arabela Tso">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Arabela's Blog">
      <meta itemprop="description" content="May all your optimal is global.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="OpenNMT 2.0.0rc1 使用手册 | Arabela's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          OpenNMT 2.0.0rc1 使用手册
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-01-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-01-03T00:00:00+08:00">2021-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-06-21 20:30:20" itemprop="dateModified" datetime="2024-06-21T20:30:20+08:00">2024-06-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
    </span>

  
    <span id="/2021/01/03/OpenNMT-Doc/" class="post-meta-item leancloud_visitors" data-flag-title="OpenNMT 2.0.0rc1 使用手册" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>最近在用OpenNMT训练机器翻译模型。OpenNMT 全称 Open-Source Neural Machine Translation，是<a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/">https://nlp.seas.harvard.edu/</a> 和<a target="_blank" rel="noopener" href="https://www.systransoft.com/">SYSTRAN</a> 共同开发的适用于机器翻译的集成库（现在由<a target="_blank" rel="noopener" href="https://www.systransoft.com/">SYSTRAN</a>和 <a target="_blank" rel="noopener" href="https://www.ubiqus.com/">Ubiqus</a> 维护）。</p>
<p>OpenNMT有两个版本，分别为依赖<a target="_blank" rel="noopener" href="https://github.com/OpenNMT/OpenNMT-py">PyTorch</a> 和<a target="_blank" rel="noopener" href="https://github.com/OpenNMT/OpenNMT-tf">TensorFlow</a>的。从使用人数上来说，PyTorch用的人多得多，更新的速度也快一点，所以就选了OpenNMT-py 的版本。据说Academia的趋势也是PyTorch增多，考虑复用的话PyTorch是你的好朋友 :p</p>
<p>不得不说，OpenNMT开发得确实快，版本更新也快，眼睁睁看着它两个月更新一个版本，也是挺6。但这也导致文档update的速度跟不上开发的速度 🐶 </p>
<p>所以本文记录下从头训练一个NMT的pipeline，也记录一些useful 的七七八八。</p>
<span id="more"></span>



<h2 id="Why-OpenNMT-2-0-0rc"><a href="#Why-OpenNMT-2-0-0rc" class="headerlink" title="Why OpenNMT 2.0.0rc"></a>Why OpenNMT 2.0.0rc</h2><p>版本更新挺快，之前的version比较稳定的是1.2.0，但最近用起来还是有点不趁手。而且2.0.0提供了pretrained tokenizer，不用白不用。所以本文之后的安装和使用，都是建立在这个版本上。</p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><h3 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h3><ul>
<li>Python &gt;&#x3D; 3.6</li>
<li>Pytorch &#x3D;&#x3D; 1.6.0</li>
</ul>
<p>接下来两种安装方法，一种pip安装，一种from source。</p>
<h3 id="From-pip"><a href="#From-pip" class="headerlink" title="From pip"></a>From pip</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install OpenNMT-py==2.0.0rc1</span><br></pre></td></tr></table></figure>

<p>如果用pip安装的时候提示MemoryError，则用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install OpenNMT-py==2.0.0rc1 --no-cache-dir</span><br></pre></td></tr></table></figure>



<h3 id="From-source"><a href="#From-source" class="headerlink" title="From source"></a>From source</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/OpenNMT/OpenNMT-py.git</span><br><span class="line">cd OpenNMT-py</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<p>如果想用pretrained model 或transformers，还要运行以下这句：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.opt.txt</span><br></pre></td></tr></table></figure>

<hr>
<p><em><strong>注意：</strong></em></p>
<p>clone 之前记得切换分支！<code>master</code> 分支是开发中的分支，如果碰上他们正在更新（是我的血泪史了QAQ），有的code 或api还没写完，很坑。</p>
<p><strong>切换方式</strong>：<code>master</code> -&gt; <code>Tags</code> -&gt; <code>2.0.0rc1</code>.</p>
<p>PS. 就在写这篇博客的当下，他们又更新到 <code>2.0.0rc2</code> 了（看了一下更新时间，14 days ago） &#x3D; &#x3D;   虽然敏捷开发是没有错，但是也太快了，文档维护没跟上，有点坑。。。</p>
<h3 id="Check-installation"><a href="#Check-installation" class="headerlink" title="Check installation"></a>Check installation</h3><p>检测是否安装成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onmt_train -h</span><br></pre></td></tr></table></figure>

<p>如果能输出help信息，说明安装成功。</p>
<h2 id="Step-0-Training-data-preparation"><a href="#Step-0-Training-data-preparation" class="headerlink" title="Step 0. Training data preparation"></a>Step 0. Training data preparation</h2><h3 id="Dataset-preparation"><a href="#Dataset-preparation" class="headerlink" title="Dataset preparation"></a>Dataset preparation</h3><p>如果选择install from source的方法，就可以看到在 <code>OpenNMT-py/data/</code> 的文件夹下已经下载好 英译德 的dataset:</p>
<ul>
<li>For training:<ul>
<li><code>src-train.txt</code></li>
<li><code>tgt-train.txt</code></li>
</ul>
</li>
<li>For validation:<ul>
<li><code>src-val.txt</code></li>
<li><code>tgt-val.txt</code></li>
</ul>
</li>
</ul>
<p>但这个dataset很小，训练效果不会好，所以这里下载更多的dataset。</p>
<p>（如果只打算跑通一个例子，可以直接跳到下一个小节，不用重新下载新的dataset）</p>
<h3 id="Dataset-Download"><a href="#Dataset-Download" class="headerlink" title="Dataset Download"></a>Dataset Download</h3><h4 id="WMT-DATASET"><a href="#WMT-DATASET" class="headerlink" title="WMT DATASET"></a>WMT DATASET</h4><p>Link: <a target="_blank" rel="noopener" href="https://www.statmt.org/wmt13/translation-task.html#download">https://www.statmt.org/wmt13/translation-task.html#download</a></p>
<p><img src="/OpenNMT-Doc/parallel.png" alt="parallel"></p>
<p>WMT 数据集，每年都有。上面的link是WMT13年的link。要找哪一年的dataset，就把 <code>wmt13</code> 这个数字改成那一年。比如要找WMT19的dataset，下载地址就是 <a target="_blank" rel="noopener" href="https://www.statmt.org/wmt19/translation-task.html#download%E3%80%82">https://www.statmt.org/wmt19/translation-task.html#download。</a></p>
<p>因为每一年翻译任务的语言不一样，所以根据要训练的翻译模型自己找年份 :(</p>
<p>下载解压之后，一般有两个文件夹。根据dataset不同，名字可能不同，但能看出来一个是源语言(<code>src</code>)，一个是目标语言(<code>tgt</code>)。文件的形式是一行一句话。</p>
<h4 id="OPUS"><a href="#OPUS" class="headerlink" title="OPUS"></a>OPUS</h4><p>Link: <a target="_blank" rel="noopener" href="http://opus.nlpl.eu/">http://opus.nlpl.eu/</a></p>
<p>这个网站也包含WMT的入口，很杂很乱，但好在可以选source，target language, customize 的空间很大。</p>
<p>下载之后的形式和WMT 的dataset很像，也是一行一句话。</p>
<p><img src="/OpenNMT-Doc/orpus.png" alt="orpus"></p>
<h4 id="Tatoeba-Kaggle"><a href="#Tatoeba-Kaggle" class="headerlink" title="Tatoeba Kaggle"></a>Tatoeba Kaggle</h4><p><a target="_blank" rel="noopener" href="http://www.manythings.org/anki/">http://www.manythings.org/anki/</a></p>
<p>这个网站的好处是简洁，一看就知道在哪里下载。</p>
<p><img src="/OpenNMT-Doc/taboeba.png" alt="orpus"></p>
<p>缺点是数据集不大，且双语放在同一个文件中，格式为<code>English + TAB + The Other Language + TAB + Attribution</code>。 如：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">This work isn&#x27;t easy.	この仕事は簡単じゃない。	CC-BY 2.0 (France) Attribution: tatoeba.org #3737550 (CK) &amp; #7977622 (Ninja)</span><br><span class="line">Those are sunflowers.	それはひまわりです。	CC-BY 2.0 (France) Attribution: tatoeba.org #441940 (CK) &amp; #205407 (arnab)</span><br><span class="line">Tom bought a new car.	トムは新車を買った。	CC-BY 2.0 (France) Attribution: tatoeba.org #1026984 (CK) &amp; #2733633 (tommy_san)</span><br><span class="line">This watch is broken.	この時計は壊れている。	CC-BY 2.0 (France) Attribution: tatoeba.org #58929 (CK) &amp; #221604 (bunbuku)</span><br></pre></td></tr></table></figure>

<p>所以下载下来需要预处理，将两种语言分开存在不同文件中，方便后续使用。</p>
<h3 id="Optional-Preprocess"><a href="#Optional-Preprocess" class="headerlink" title="(Optional) Preprocess"></a>(Optional) Preprocess</h3><p>一般上述网站提供的数据集已经进过一定程度的清洗，但还可以根据自己的需求继续做一些预处理。这里不做赘述，可根据需要处理。</p>
<p><em><strong>注意：</strong></em></p>
<p>OpenNMT 2.0.0rc1 提供去除长句、特殊词汇等功能，可以不用自己处理，美滋滋 :p</p>
<h2 id="Step-1-Build-Vocabulary"><a href="#Step-1-Build-Vocabulary" class="headerlink" title="Step 1. Build Vocabulary"></a>Step 1. Build Vocabulary</h2><p>准备好数据集之后，先建立词汇表。</p>
<p>OpenNMT提供<code>onmt_build_vocab</code> 命令，输入<code>onmt_build_vocab</code>可以看到所有的arguments。为了避免每次输入一堆argument的麻烦，OpenNMT 可以接收一整份configuration作为输入，这样就省去了很多麻烦。</p>
<h3 id="Example-configuration-for-Build-Vocab"><a href="#Example-configuration-for-Build-Vocab" class="headerlink" title="Example configuration for Build Vocab"></a>Example configuration for Build Vocab</h3><p>This is the example provided <a target="_blank" rel="noopener" href="https://opennmt.net/OpenNMT-py/examples/Translation.html">here</a>. 这个例子里用作分词的语言模型是subword，需要提前训练， 保存在<code>data/wmt/wmtende.model</code>。如果不想做这一步，可以直接看下一个例子。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wmt14_en_de.yaml</span></span><br><span class="line"><span class="attr">save_data:</span> <span class="string">data/wmt/run/example</span></span><br><span class="line"><span class="comment">## Where the vocab(s) will be written</span></span><br><span class="line"><span class="attr">src_vocab:</span> <span class="string">data/wmt/run/example.vocab.src</span></span><br><span class="line"><span class="attr">tgt_vocab:</span> <span class="string">data/wmt/run/example.vocab.tgt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus opts:</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">    <span class="attr">commoncrawl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/commoncrawl.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/commoncrawl.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [<span class="string">sentencepiece</span>, <span class="string">filtertoolong</span>]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">23</span></span><br><span class="line">    <span class="attr">europarl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/europarl-v7.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/europarl-v7.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [<span class="string">sentencepiece</span>, <span class="string">filtertoolong</span>]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">19</span></span><br><span class="line">    <span class="attr">news_commentary:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/news-commentary-v11.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/news-commentary-v11.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [<span class="string">sentencepiece</span>, <span class="string">filtertoolong</span>]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">valid:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/valid.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/valid.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [<span class="string">sentencepiece</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### Transform related opts:</span></span><br><span class="line"><span class="comment">#### Subword</span></span><br><span class="line"><span class="attr">src_subword_model:</span> <span class="string">data/wmt/wmtende.model</span></span><br><span class="line"><span class="attr">tgt_subword_model:</span> <span class="string">data/wmt/wmtende.model</span></span><br><span class="line"><span class="attr">src_subword_nbest:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">src_subword_alpha:</span> <span class="number">0.0</span></span><br><span class="line"><span class="attr">tgt_subword_nbest:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">tgt_subword_alpha:</span> <span class="number">0.0</span></span><br><span class="line"><span class="comment">#### Filter</span></span><br><span class="line"><span class="attr">src_seq_length:</span> <span class="number">150</span></span><br><span class="line"><span class="attr">tgt_seq_length:</span> <span class="number">150</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># silently ignore empty lines in the data</span></span><br><span class="line"><span class="attr">skip_empty_level:</span> <span class="string">silent</span></span><br></pre></td></tr></table></figure>



<p>以一个不用语言模型的分词作例子：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wmt14_en_de.yaml</span></span><br><span class="line"><span class="attr">save_data:</span> <span class="string">data/wmt/run/example</span></span><br><span class="line"><span class="comment">## Where the vocab(s) will be written</span></span><br><span class="line"><span class="attr">src_vocab:</span> <span class="string">data/wmt/run/example.vocab.src</span></span><br><span class="line"><span class="attr">tgt_vocab:</span> <span class="string">data/wmt/run/example.vocab.tgt</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Corpus opts:</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">    <span class="attr">commoncrawl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/commoncrawl.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/commoncrawl.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [ ]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">23</span></span><br><span class="line">    <span class="attr">europarl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/europarl-v7.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/europarl-v7.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [ ]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">19</span></span><br><span class="line">    <span class="attr">news_commentary:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/news-commentary-v11.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/news-commentary-v11.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [ ]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">valid:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/valid.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/valid.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [ ]</span><br><span class="line"> </span><br><span class="line"><span class="attr">src_vocab_size:</span> <span class="number">32000</span></span><br><span class="line"><span class="attr">tgt_vocab_size:</span> <span class="number">32000</span></span><br><span class="line"><span class="attr">vocab_size_multiple:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">src_words_min_frequency:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">tgt_words_min_frequency:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">share_vocab:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>这个例子用到四个数据集，分别设置了不同的权重，src和tgt语言共享一个大小为32000的单词表，单词表存在<code>src_vocab</code>和<code>tgt_vocab</code>指定的路径下。</p>
<p>保存这个config文件，运行：(注意替换<code>url-to-config-file</code> 成存储的路径, 如 <code>config/build_vocab.yaml</code>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onmt_build_vocab -config url-to-config-file -n_sample -1</span><br></pre></td></tr></table></figure>

<p>这里n_sample 是用到的数据数量，-1指用全部。</p>
<h3 id="Data-Transformer"><a href="#Data-Transformer" class="headerlink" title="Data Transformer"></a>Data Transformer</h3><p>OpenNMT 2.0 以后提供常用的预处理方法和分词模型，如去除长句，bpe等。</p>
<p><strong>用法</strong></p>
<p>在每个dataset的transform中填写preset transformer，例如用<code>sentencepiece</code> 做分词；</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data:</span></span><br><span class="line">    <span class="attr">commoncrawl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/commoncrawl.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/commoncrawl.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [<span class="string">sentencepiece</span>]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">23</span></span><br></pre></td></tr></table></figure>



<h4 id="Preprocess-Filterlong"><a href="#Preprocess-Filterlong" class="headerlink" title="Preprocess - Filterlong"></a>Preprocess - Filterlong</h4><p>用<code>filterlong</code>做预处理去掉过长句子，搭配以下comment一起使用：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Filter</span></span><br><span class="line"><span class="attr">src_seq_length:</span> <span class="number">150</span></span><br><span class="line"><span class="attr">tgt_seq_length:</span> <span class="number">150</span></span><br></pre></td></tr></table></figure>



<p>不一一赘述了。</p>
<p>注意：sentencepiece如果</p>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><h5 id="No-special-tokenization"><a href="#No-special-tokenization" class="headerlink" title="No special tokenization."></a>No special tokenization.</h5><p>相当于直接用空格当作tokenization. 缺点是vocabulary会很大，很多同源词会被当作独立的单词。好处是简单，无需用到什么语言模型。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data:</span></span><br><span class="line">    <span class="attr">commoncrawl:</span></span><br><span class="line">        <span class="attr">path_src:</span> <span class="string">data/wmt/commoncrawl.de-en.en</span></span><br><span class="line">        <span class="attr">path_tgt:</span> <span class="string">data/wmt/commoncrawl.de-en.de</span></span><br><span class="line">        <span class="attr">transforms:</span> [ ]</span><br><span class="line">        <span class="attr">weight:</span> <span class="number">23</span></span><br></pre></td></tr></table></figure>

<h5 id="Sentencepiece"><a href="#Sentencepiece" class="headerlink" title="Sentencepiece"></a>Sentencepiece</h5><p>Sentencepiece要预训练一个语言模型，然后用语言模型将source 和target文件进行分词，保存相应的vocabulary。</p>
<p><strong>(a) 训练语言模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.Train(<span class="built_in">input</span>=<span class="string">&#x27;url-to-your-source-or-target-file&#x27;</span>,</span><br><span class="line">                               model_prefix=<span class="string">&#x27;/data/model&#x27;</span>,</span><br><span class="line">                               vocab_size=<span class="number">32000</span>,</span><br><span class="line">                               character_coverage=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>参数<code>input</code> 和<code>model_prefix</code>要指定, <code>vocab_size</code> 可以自定义，这里设置为32000。</p>
<p>(b) <strong>Encode</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spp = spm.SentencePieceProcessor()</span><br><span class="line">spp.Load(model_file=<span class="string">&#x27;url-to-model-you-trained&#x27;</span>)</span><br><span class="line">output = spp.encode(line, out_type=<span class="string">&quot;str&quot;</span>) <span class="comment"># here, `line&#x27; should be specified, can be read from file.</span></span><br></pre></td></tr></table></figure>

<p><code>model_file</code>指定到刚刚训练好的语言模型路径，<code>line</code> 是待encoding的句子&#x2F;段落。</p>
<p><strong>(c) Decode (After translation)</strong></p>
<p>训练模型之前不需要做这一步，这一步是提供给训练模型之后的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spp = spm.SentencePieceProcessor()</span><br><span class="line">spp.Load(model_file=<span class="string">&#x27;url-to-model-you-trained&#x27;</span>)</span><br><span class="line">output = spp.decode(line)</span><br></pre></td></tr></table></figure>

<p><code>model_file</code>指定到刚刚训练好的语言模型路径，<code>line</code> 是待decoding的句子列表。</p>
<h5 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h5><p>在<code>tools/</code>下有<code>bpe_pipeline.sh</code> 可以直接用。</p>
<p>其他可用的data transformers 在<a target="_blank" rel="noopener" href="https://opennmt.net/OpenNMT-py/FAQ.html?highlight=transformer#what-are-the-readily-available-on-the-fly-data-transforms">这里</a>，</p>
<h2 id="Step-2-Train-the-model"><a href="#Step-2-Train-the-model" class="headerlink" title="Step 2. Train the model"></a>Step 2. Train the model</h2><p>把下面的configuration加在之前build vocabulary 的config 后面：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># General opts</span></span><br><span class="line"><span class="attr">save_model:</span> <span class="string">data/wmt/run/model</span></span><br><span class="line"><span class="attr">keep_checkpoint:</span> <span class="number">50</span></span><br><span class="line"><span class="attr">save_checkpoint_steps:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">average_decay:</span> <span class="number">0.0005</span></span><br><span class="line"><span class="attr">seed:</span> <span class="number">1234</span></span><br><span class="line"><span class="attr">report_every:</span> <span class="number">100</span></span><br><span class="line"><span class="attr">train_steps:</span> <span class="number">100000</span></span><br><span class="line"><span class="attr">valid_steps:</span> <span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Batching</span></span><br><span class="line"><span class="attr">queue_size:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">bucket_size:</span> <span class="number">32768</span></span><br><span class="line"><span class="attr">world_size:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">gpu_ranks:</span> [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="attr">batch_type:</span> <span class="string">&quot;tokens&quot;</span></span><br><span class="line"><span class="attr">batch_size:</span> <span class="number">4096</span></span><br><span class="line"><span class="attr">valid_batch_size:</span> <span class="number">16</span></span><br><span class="line"><span class="attr">batch_size_multiple:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">max_generator_batches:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">accum_count:</span> [<span class="number">3</span>]</span><br><span class="line"><span class="attr">accum_steps:</span> [<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimization</span></span><br><span class="line"><span class="attr">model_dtype:</span> <span class="string">&quot;fp32&quot;</span></span><br><span class="line"><span class="attr">optim:</span> <span class="string">&quot;adam&quot;</span></span><br><span class="line"><span class="attr">learning_rate:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">warmup_steps:</span> <span class="number">8000</span></span><br><span class="line"><span class="attr">decay_method:</span> <span class="string">&quot;noam&quot;</span></span><br><span class="line"><span class="attr">adam_beta2:</span> <span class="number">0.998</span></span><br><span class="line"><span class="attr">max_grad_norm:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">label_smoothing:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">param_init:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">param_init_glorot:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">normalization:</span> <span class="string">&quot;tokens&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="attr">encoder_type:</span> <span class="string">transformer</span></span><br><span class="line"><span class="attr">decoder_type:</span> <span class="string">transformer</span></span><br><span class="line"><span class="attr">enc_layers:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">dec_layers:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">heads:</span> <span class="number">8</span></span><br><span class="line"><span class="attr">rnn_size:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">word_vec_size:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">transformer_ff:</span> <span class="number">2048</span></span><br><span class="line"><span class="attr">dropout_steps:</span> [<span class="number">0</span>]</span><br><span class="line"><span class="attr">dropout:</span> [<span class="number">0.1</span>]</span><br><span class="line"><span class="attr">attention_dropout:</span> [<span class="number">0.1</span>]</span><br><span class="line"><span class="attr">share_decoder_embeddings:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">share_embeddings:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li><code>gpu_ranks</code>： 指定前两个gpu core，这里根据需要设置要用的gpu序号和个数。</li>
<li>这里的模型是 encoder，decoder都为6层transformer 的attention model，各种参数都可调</li>
<li>如果想从某个checkpoint开始train起，加上参数<code>train_from: path-to-pt-file</code></li>
<li>可以指定log路径：<code>log_file: path-you-want-to-save-log-file</code></li>
<li>用几个GPU进行训练，<code>world_size</code> 就设置成几。</li>
<li>如果<code>share_vocab: false</code>, 则 <code>share_decoder_embeddings</code> 也要为false。</li>
</ul>
<h2 id="Step-3-Translate"><a href="#Step-3-Translate" class="headerlink" title="Step 3. Translate"></a>Step 3. Translate</h2><p><strong>用法</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onmt_translate -src src-file -tgt tgt-file -model url-to-the-model -replace_unk -gpu 0 -verbose</span><br></pre></td></tr></table></figure>

<p><strong>输出</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[2020-12-29 12:03:14,773 INFO]</span><br><span class="line">SENT 178: [&#x27;The&#x27;, &#x27;governing&#x27;, &#x27;Justice&#x27;, &#x27;and&#x27;, &#x27;Development&#x27;, &#x27;party&#x27;, &#x27;-&#x27;, &#x27;the&#x27;, &#x27;AKP&#x27;, &#x27;-&#x27;, &#x27;has&#x27;, &#x27;crafted&#x27;, &#x27;a&#x27;, &#x27;whole&#x27;, &#x27;new&#x27;, &#x27;foreign&#x27;, &#x27;policy&#x27;, &#x27;for&#x27;, &#x27;the&#x27;, &#x27;country.&#x27;]</span><br><span class="line">PRED 178: Toda la política exterior y de desarrollo ha crafted un nuevo partido para el país whole</span><br><span class="line">PRED SCORE: -15.3076</span><br><span class="line">GOLD 178: El Partido de Justicia y Desarrollo que está al gobierno - el &lt;unk&gt; - ha planeado una política exterior nueva para el país.</span><br><span class="line">GOLD SCORE: -61.9490</span><br><span class="line"></span><br><span class="line">[2020-12-29 12:03:14,773 INFO]</span><br><span class="line">SENT 179: [&#x27;The&#x27;, &#x27;relative&#x27;, &#x27;isolation&#x27;, &#x27;from&#x27;, &#x27;its&#x27;, &#x27;surrounding&#x27;, &#x27;region,&#x27;, &#x27;engendered&#x27;, &#x27;by&#x27;, &#x27;the&#x27;, &#x27;frozen&#x27;, &#x27;boundaries&#x27;, &#x27;of&#x27;, &#x27;the&#x27;, &#x27;Cold&#x27;, &#x27;War,&#x27;, &#x27;has&#x27;, &#x27;gone.&#x27;]</span><br><span class="line">PRED 179: La Guerra Fría, frozen por su aislamiento relativo de la región, ha frozen los límites de la Cold</span><br><span class="line">PRED SCORE: -13.5960</span><br><span class="line">GOLD 179: El relativo aislamiento de las regiones &lt;unk&gt; generado por las fronteras &lt;unk&gt; de la Guerra Fría, ha desaparecido.</span><br><span class="line">GOLD SCORE: -50.6623</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：</p>
<ul>
<li>这里的<code>src-file</code>, <code>tgt-file</code>, <code>url-to-the-model</code> 需指定</li>
<li>可以用多个gpu进行计算，用法<code>-gpu 0 1</code></li>
<li>如果不使用<code>-replace_unk</code>,预测出来的句子就都是 <UNK> </li>
<li><code>-verbose</code>打印出每个句子的具体翻译情况</li>
<li>如果用了语言模型做分词，要先将source 和target 文件用相应的语言模型进行处理，再translate。</li>
</ul>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>文档不是很完善，但是看代码还是可以。版本迭代很快，well-maintained，看看代码还是能跟上的。总体来说是个提高效率的codebook吧。</p>
<p>BTW，内存管理有些问题，不确定问题出在OpenNMT还是pytorch上。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpeg" alt="Arabela Tso 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpeg" alt="Arabela Tso 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>


        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/01/09/Install%20Anaconda%20on%20CentOS%20without%20root/" rel="prev" title="Install Anaconda without root privilege on CentOS">
                  <i class="fa fa-angle-left"></i> Install Anaconda without root privilege on CentOS
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/06/21/latex-tips-0/some-english-translations/" rel="next" title="常用英文翻译（简历相关）">
                  常用英文翻译（简历相关） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Arabela Tso</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/ArabelaTso" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  





  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"naaXkdnpYPUXOj8VlETpQkYr-gzGzoHsz","app_key":"N2cpd0QyqVvPeOafJtl4s0zH","security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>



</body>
</html>
